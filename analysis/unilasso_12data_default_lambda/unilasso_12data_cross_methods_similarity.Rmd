```{r setup}
if (R.home() != "/scratch/users/aqwang/conda/envs/r_package/lib/R") {
  system("/scratch/users/aqwang/conda/envs/r_package/bin/Rscript -e 'rmarkdown::render(\"unilasso_12data_cross_methods_similarity.Rmd\")'")
  quit("no")
}

# check the R environment is r_package
R.home()

# set working directory
setwd("/accounts/grad/aqwang/unilasso/analysis/unilasso_12data_default_lambda")
# load packages 
library(uniLasso) # for unilasso 
library(glmnet) # for cv lasso 
library(dplyr)
library(tidyr)

# Define all dataset names (remove "data_" prefix)
dataset_names <- c(
  "ca_housing", "computer",
  "debutanizer", "diamond", "elevator", "energy_efficiency",
  "insurance", "kin8nm", "miami_housing", "naval_propulsion",
 "protein_structure", "qsar")

```


```{r }
# Function to run analysis for a single split - Feature Similarity Version (with Least Squares and Coefficients)
run_single_split_similarity <- function(split_id, X, y, dataset_name) {
  # Split data into training and test sets (different seed for each split)
  set.seed(split_id)
  # change the training size here if needed
  training_ratio <- 0.5
  training_size <- floor(training_ratio * nrow(X))
  train_indices <- sample(seq_len(nrow(X)), size = training_size)
  Xtr <- X[train_indices, ]
  ytr <- y[train_indices]
  Xte <- X[-train_indices, ]
  yte <- y[-train_indices]
  
  # Initialize results for this split
  split_results <- list()
  
  # ensuring the same CV folds for all methods within this split
  set.seed(42 + split_id)
  K <- 10
  n <- nrow(Xtr)
  foldid <- sample(rep(1:K, length.out = n))
  
  # Store selected features and coefficients for each method
  selected_features <- list()
  coefficients <- list()  # NEW: Store coefficients
  
  # 1. UniLasso with loo=TRUE
  tryCatch({
    unilasso_cv_loo_true <- cv.uniLasso(
      Xtr, ytr,
      family = "gaussian",
      loo = TRUE,
      lower.limits = 0,
      standardize = FALSE,
      foldid = foldid,
      nlambda = 100
    )
    
    coefs <- coef(unilasso_cv_loo_true, s = "lambda.min")[-1]  # exclude intercept
    selected_features$unilasso_loo_true <- which(coefs != 0)
    coefficients$unilasso_loo_true <- as.numeric(coefs)  # NEW: Store coefficients
    
  }, error = function(e) {
    selected_features$unilasso_loo_true <- integer(0)  # empty if error
    coefficients$unilasso_loo_true <- rep(NA_real_, ncol(Xtr))  # NEW
  })
  
  # 2. Polish UniLasso
  tryCatch({
    pol <- polish.uniLasso(Xtr, ytr,
                          family = "gaussian",
                          foldid = foldid,
                          nlambda = 100, 
                          standardize = FALSE,
                          intercept = TRUE,
                          loo = TRUE)
    
    coefs <- coef(pol, s = "lambda.min")[-1]  # exclude intercept
    selected_features$polish_unilasso <- which(coefs != 0)
    coefficients$polish_unilasso <- as.numeric(coefs)  # NEW: Store coefficients
    
  }, error = function(e) {
    selected_features$polish_unilasso <- integer(0)
    coefficients$polish_unilasso <- rep(NA_real_, ncol(Xtr))  # NEW
  })
  
  # 3. Lasso with CV
  tryCatch({
    lasso_cv <- cv.glmnet(
      Xtr, ytr,
      family = "gaussian",
      alpha = 1,
      standardize = TRUE,
      foldid = foldid,
      nlambda = 100
    )
    
    coefs <- coef(lasso_cv, s = "lambda.min")[-1]  # exclude intercept
    selected_features$lasso_cv <- which(coefs != 0)
    coefficients$lasso_cv <- as.numeric(coefs)  # NEW: Store coefficients
    
  }, error = function(e) {
    selected_features$lasso_cv <- integer(0)
    coefficients$lasso_cv <- rep(NA_real_, ncol(Xtr))  # NEW
  })
  
  # 4. UniReg with CV
  tryCatch({
    uniReg_cv <- cv.uniReg(
      Xtr, ytr,
      family = "gaussian",
      standardize = FALSE,
      foldid = foldid,
      nlambda = 100,
      loo = TRUE
    )
    
    coefs <- coef(uniReg_cv, s = 0)[-1]  # s=0, no regularization 
    selected_features$unireg <- which(coefs != 0)
    coefficients$unireg <- as.numeric(coefs)  # NEW: Store coefficients
    
  }, error = function(e) {
    selected_features$unireg <- integer(0)
    coefficients$unireg <- rep(NA_real_, ncol(Xtr))  # NEW
  })
  
  # 5. Least Squares Regression (no regularization)
  tryCatch({
    # Convert to data frames for lm()
    Xtr_df <- as.data.frame(Xtr)
    
    ls_fit <- lm(ytr ~ ., data = Xtr_df)  # use . to include all variables
    
    # Get coefficients and find non-zero ones (excluding intercept)
    coefs <- coef(ls_fit)[-1]  # exclude intercept
    selected_features$least_squares <- which(!is.na(coefs) & coefs != 0)
    coefficients$least_squares <- as.numeric(coefs)  # NEW: Store coefficients (includes NA)
    
  }, error = function(e) {
    selected_features$least_squares <- integer(0)
    coefficients$least_squares <- rep(NA_real_, ncol(Xtr))  # NEW
  })
  
  # Function to calculate Jaccard similarity (intersection/union)
  jaccard_similarity <- function(set1, set2) {
    if (length(set1) == 0 && length(set2) == 0) {
      return(1)  # Both empty sets are considered 100% similar
    }
    intersection <- length(intersect(set1, set2))
    union <- length(union(set1, set2))
    return(intersection / union)
  }
  
  # Calculate pairwise similarities between all methods
  method_names <- names(selected_features)
  
  # Generate all pairwise combinations
  for (i in 1:(length(method_names) - 1)) {
    for (j in (i + 1):length(method_names)) {
      method1 <- method_names[i]
      method2 <- method_names[j]
      
      similarity <- jaccard_similarity(
        selected_features[[method1]], 
        selected_features[[method2]]
      )
      
      # Create column name for this pair
      pair_name <- paste0("similarity_", method1, "_vs_", method2)
      split_results[[pair_name]] <- similarity
    }
  }
  
  # Also store the number of selected features for each method
  for (method in method_names) {
    split_results[[paste0(method, "_num_features")]] <- length(selected_features[[method]])
  }
  
  # Add dataset and split information
  split_results$dataset <- dataset_name
  split_results$split_id <- split_id
  
  # NEW: Return both similarity results and coefficients
  return(list(
    similarity_results = split_results,
    coefficients = coefficients,
    selected_features = selected_features,
    dataset_info = list(
      dataset = dataset_name,
      split_id = split_id,
      n_features = ncol(Xtr),
      n_observations = nrow(Xtr)
    )
  ))
}

# Now the similarity columns will be (10 total):
# similarity_unilasso_loo_true_polish_unilasso
# similarity_unilasso_loo_true_lasso_cv
# similarity_unilasso_loo_true_unireg
# similarity_unilasso_loo_true_least_squares
# similarity_polish_unilasso_lasso_cv
# similarity_polish_unilasso_unireg
# similarity_polish_unilasso_least_squares
# similarity_lasso_cv_unireg
# similarity_lasso_cv_least_squares
# similarity_unireg_least_squares

# Plus feature count columns (5 total):
# unilasso_loo_true_num_features
# polish_unilasso_num_features
# lasso_cv_num_features
# unireg_num_features
# least_squares_num_features
```

# loop through all datasets and one split for each dataset
```{r run-analysis}
# loop through all datasets and one split for each dataset
all_results <- list()
all_coefficients <- list()  # NEW: Store all coefficients

for (i in 1:length(dataset_names)) {  # Add index i
  dataset_name <- dataset_names[i]    # Now i is defined
  
  cat("Processing dataset:", dataset_name, "\n")  # Add progress tracking
  
  # Load data
  X_path <- paste0("/accounts/grad/aqwang/unilasso/datasets_12/", dataset_name, "/X.csv")
  y_path <- paste0("/accounts/grad/aqwang/unilasso/datasets_12/", dataset_name, "/y.csv")
  
  tryCatch({
    X <- read.csv(X_path)
    X <- as.matrix(X)
    y <- read.csv(y_path, header = FALSE)
    y <- as.numeric(unlist(y))
    
    cat("Dataset loaded - X:", nrow(X), "x", ncol(X), ", y:", length(y), "\n")
    
    # Set seed for reproducible train/test split
    set.seed(123)
    training_ratio <- 0.5
    training_size <- floor(training_ratio * nrow(X))
    train_indices <- sample(seq_len(nrow(X)), size = training_size)
    Xtr <- X[train_indices, ]
    ytr <- y[train_indices]
    
    split_id <- 1  # Only one split per dataset
    result <- run_single_split_similarity(split_id, Xtr, ytr, dataset_name)
    
    # Extract similarity results and coefficients
    similarity_result <- result$similarity_results
    coefficient_result <- result$coefficients
    
    # Convert similarity result to data frame
    result_df <- data.frame(similarity_result, stringsAsFactors = FALSE)
    all_results[[dataset_name]] <- result_df
    
    # Store coefficients with dataset info
    all_coefficients[[dataset_name]] <- list(
      coefficients = coefficient_result,
      selected_features = result$selected_features,
      dataset_info = result$dataset_info
    )
    
    cat("Dataset", dataset_name, "completed successfully\n\n")
    
  }, error = function(e) {
    cat("ERROR processing dataset", dataset_name, ":", e$message, "\n\n")
    all_results[[dataset_name]] <- NULL
    all_coefficients[[dataset_name]] <- NULL
  })
}

# Combine all results into a single data frame
cat("Combining results from all datasets...\n")

# Filter out any NULL results (failed datasets)
valid_results <- all_results[!sapply(all_results, is.null)]
valid_coefficients <- all_coefficients[!sapply(all_coefficients, is.null)]

if (length(valid_results) > 0) {
  # Combine all results into one data frame
  final_results <- do.call(rbind, valid_results)
  rownames(final_results) <- NULL  # Clean up row names
  
  cat("Final combined results:\n")
  cat("Shape:", nrow(final_results), "rows x", ncol(final_results), "columns\n")
  cat("Datasets included:", paste(unique(final_results$dataset), collapse = ", "), "\n\n")
  
  # Display the results
  print(final_results)
  
} else {
  cat("No valid results to combine!\n")
  final_results <- NULL
}

# Save and analyze results
if (!is.null(final_results)) {
  
  # Create results directory if it doesn't exist
  if (!dir.exists("similarity_results")) {
    dir.create("similarity_results", recursive = TRUE)
  }
  
  # Save similarity results to CSV
  write.csv(final_results, "similarity_results/feature_similarity_12datasets.csv", row.names = FALSE)
  cat("Similarity results saved to: similarity_results/feature_similarity_12datasets.csv\n")
  
  # NEW: Save coefficients to RData
  save(valid_coefficients, file = "similarity_cross_methods_results/coefficients_12datasets_one_split.RData")
  cat("Coefficients saved to: similarity_cross_methods_results/coefficients_12datasets.RData\n")
  
  # NEW: Also save a summary of coefficients
  coefficient_summary <- list()
  for (dataset in names(valid_coefficients)) {
    coef_data <- valid_coefficients[[dataset]]
    coefficient_summary[[dataset]] <- list(
      dataset_info = coef_data$dataset_info,
      method_names = names(coef_data$coefficients),
      coefficient_lengths = sapply(coef_data$coefficients, length),
      selected_feature_counts = sapply(coef_data$selected_features, length)
    )
  }
  
  save(coefficient_summary, file = "similarity_cross_methods_results/coefficient_summary_12datasets.RData")
  cat("Coefficient summary saved to: similarity_cross_methods_results/coefficient_summary_12datasets.RData\n")
}
```


```{r load the results}
#read csv file
results <- read.csv("similarity_cross_methods_results/feature_similarity_12datasets.csv")

# choose the first 10 column for similarity and last two column for dataset and split_id
similarity_columns <- grep("^similarity_", colnames(results), value = TRUE)
results_subset <- results[, c("dataset", similarity_columns)]

# round up to two decimal places
results_subset[similarity_columns] <- round(results_subset[similarity_columns], 2)

# rename the similarity columns to remove "similarity_" prefix
colnames(results_subset) <- gsub("^similarity_", "", colnames(results_subset))

#rename all methods to shorter names
colnames(results_subset) <- gsub("unilasso_loo_true", "UniLasso", colnames(results_subset))
colnames(results_subset) <- gsub("polish_unilasso", "Polish", colnames(results_subset))
colnames(results_subset) <- gsub("lasso_cv", "Lasso", colnames(results_subset))
colnames(results_subset) <- gsub("unireg", "UniReg", colnames(results_subset))
colnames(results_subset) <- gsub("least_squares", "LS", colnames(results_subset))

results_subset$dataset <- recode(results_subset$dataset,
                            "ca_housing" = "CA housing",
                            "debutanizer" = "Debutanizer",
                            "insurance" = "Insurance",
                            "kin8nm" = "Kin8nm",
                            "computer" = "Computer",
                            "elevator" = "Elevator",
                            "energy_efficiency" = "Energy efficiency",
                            "miami_housing" = "Miami housing",
                            "naval_propulsion" = "Naval propulsion",
                            "diamond" = "Diamond",
                            "protein_structure" = "Protein structure",
                            "qsar" = "QSAR"
)

# reorder the dataset in the results_subset
results_subset$dataset <- factor(results_subset$dataset, levels = c("CA housing", "Debutanizer", "Insurance", "Kin8nm", "Computer", "Energy efficiency","Miami housing", "Protein structure", "QSAR", "Elevator", "Diamond", "Naval propulsion"))
results_subset <- results_subset %>% arrange(dataset)

```


# graphing
```{r look at coeffficents data}
# Load coefficients data
load("similarity_cross_methods_results/coefficients_12datasets_one_split.RData") # the name is valid_coefficients

# for each dataset, print out the number of selected features for each method and the number of features in common
for (dataset in names(valid_coefficients)) {
  cat("Dataset:", dataset, "\n")
  coef_data <- valid_coefficients[[dataset]]
  dataset_info <- coef_data$dataset_info  
  n_features <- dataset_info$n_features
  n_observations <- dataset_info$n_observations
  cat("Number of features:", n_features, "\n")
  cat("Number of observations:", n_observations, "\n")
  for (method in names(coef_data$coefficients)) {
    n_selected <- length(coef_data$selected_features[[method]])
    cat("  Method:", method, "- Selected features:", n_selected, "\n")
  }
  cat("\n")
}


# for each dataset, make a dataframe where the column and row name is the method name and the number of selected features, and the cell value is the number of features in common
# For each dataset, create and save a matrix of number of features in common between methods
common_features_matrices <- list()
for (dataset in names(valid_coefficients)) {
  cat("Dataset:", dataset, "\n")
  coef_data <- valid_coefficients[[dataset]]
  method_names <- names(coef_data$coefficients)
  n_methods <- length(method_names)
  common_matrix <- matrix(0, nrow = n_methods, ncol = n_methods)
  # Create method names with number of selected features
  method_names_with_counts <- sapply(method_names, function(m) {
    n_selected <- length(coef_data$selected_features[[m]])
    clean_name <- switch(m,
                        "unilasso_loo_true" = "UniLasso",
                        "polish_unilasso" = "Polish", 
                        "lasso_cv" = "Lasso",
                        "unireg" = "UniReg",
                        "least_squares" = "LS",
                        m)  # fallback to original name
    paste0(clean_name, " (","support=", n_selected, ")")
  })
  rownames(common_matrix) <- method_names_with_counts
  colnames(common_matrix) <- method_names_with_counts
  for (i in 1:n_methods) {
    for (j in 1:n_methods) {
      method1 <- method_names[i]
      method2 <- method_names[j]
      common_features <- length(intersect(coef_data$selected_features[[method1]], coef_data$selected_features[[method2]]))
      common_matrix[i, j] <- common_features
    }
  }
  # Save the matrix in a list
  common_features_matrices[[dataset]] <- common_matrix
  print(common_matrix)
}


# Save the items in common features matrices as dataframes
for (dataset in names(common_features_matrices)) {
  df <- as.data.frame(common_features_matrices[[dataset]])
  write.csv(df, file = paste0("similarity_cross_methods_results/common_features_table_onesplit_", dataset, ".csv"), row.names = TRUE)
}


library(ggplot2)
library(dplyr)
library(tidyr)
library(purrr)
library(patchwork)
library(stringr)

names(common_features_matrices) 

df_long <- map_dfr(names(common_features_matrices), function(name) {
  mat <- common_features_matrices[[name]]
  as.data.frame(mat) %>%
    mutate(Row = rownames(mat)) %>%
    pivot_longer(cols = -Row, names_to = "Col", values_to = "Value") %>%
    mutate(Dataset = name)
})

# rename the dataset
df_long$Dataset <- recode(df_long$Dataset,
                          "ca_housing" = "CA housing",
                          "debutanizer" = "Debutanizer",
                          "insurance" = "Insurance",
                          "kin8nm" = "Kin8nm",
                          "computer" = "Computer",
                          "elevator" = "Elevator",
                          "energy_efficiency" = "Energy efficiency",
                          "miami_housing" = "Miami housing",
                          "naval_propulsion" = "Naval propulsion",
                          "diamond" = "Diamond",  
                          "protein_structure" = "Protein structure",
                          "qsar" = "QSAR"
)

#rearrange based on predefined order
df_long$Dataset <- factor(df_long$Dataset, levels = c("CA housing", "Debutanizer", "Insurance", "Kin8nm", "Computer", "Energy efficiency","Miami housing", "Protein structure", "QSAR", "Elevator", "Diamond", "Naval propulsion"))
df_long <- df_long %>% arrange(Dataset)

#remove the (support=number) from Col for better visualization
df_long$Col <- gsub(" \\(support=.*\\)", "", df_long$Col)
df_long$Row <- gsub(" \\(support=.*\\)", "", df_long$Row)

# for each dataset, make a heatmap of the number of features in common between methods with numbers in the cells

# Create all plots first and store them in a list
plot_list <- list()

# Define base method order
method_levels <- c("LS","Lasso","UniReg","UniLasso","Polish")

for (dataset in unique(df_long$Dataset)) {
  df_subset <- df_long %>% filter(Dataset == dataset) %>%
    mutate(
    Row_base = str_extract(Row, paste(method_levels, collapse = "|")),
    Col_base = str_extract(Col, paste(method_levels, collapse = "|"))
  )
    row_lvls <- unique(df_subset$Row[order(match(df_subset$Row_base, method_levels))])
    col_lvls <- unique(df_subset$Col[order(match(df_subset$Col_base, method_levels))])

    df_subset <- df_subset %>%
    mutate(
      Row = factor(Row, levels = row_lvls),
      Col = factor(Col, levels = col_lvls)
    ) %>%
    # keep only upper triangle (including diagonal)
    filter(as.numeric(Row) >= as.numeric(Col))  # as.numric(Row) means the position of Row in the factor levels
  
  # Get the position of current dataset
  dataset_position <- which(unique(df_long$Dataset) == dataset)
  
  # Determine if this is in the bottom row (datasets 9, 10, 11, 12)
  is_bottom_row <- dataset_position > 8
  
  # Determine if y-axis should be shown (only for positions 1, 5, 9)
  show_y_axis <- dataset_position %in% c(1, 5, 9)
  
  p <- ggplot(df_subset, aes(x = Col, y = Row, fill = Value)) +
    geom_tile(color = "#2e2c2c") + #base tiles with light gray borders
    # highlight diagonal
    geom_tile(
      data = df_subset %>% filter(Row == Col),
      color = "black", size = 1.2, fill = NA
    ) +
    geom_text(aes(label = Value), color = "black", size = 6, face="bold", family="Times New Roman") +
    scale_fill_gradient(low = "white", high = "#e58de4") +
    scale_y_discrete(limits = rev(levels(df_subset$Row))) +  # <---- flip y axis
    coord_fixed(ratio = 1) +   # <- makes each tile square
    theme_minimal() +
    theme(
      # white backgrounds everywhere
      panel.background = element_rect(fill = "white", colour = NA),
      plot.background  = element_rect(fill = "white", colour = NA),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),

      plot.margin = margin(t = 10, r = 10, b = 30, l = 10), # adds space below each plot

      axis.text.x = if (is_bottom_row) {
        element_text(angle = 45, hjust = 1, size = 18, face = "bold", family="Times New Roman")
      } else {
        element_blank()  # Remove x-axis text for first 9 datasets
      },
      
      # Conditionally show/hide y-axis text
      axis.text.y = if (show_y_axis) {
        element_text(size = 18, face = "bold", family="Times New Roman")
      } else {
        element_blank()  # Remove y-axis text for positions 2,3,4,6,7,8,10,11,12
      },
      
      axis.title = element_text(size = 16, face = "bold", family="Times New Roman"),
      plot.title = element_text(size = 16, face = "bold", family="Times New Roman"),
      legend.position = "none"
    ) +
    annotate("text", x = "UniLasso", y = "LS", vjust = -1,
            label = dataset,
            size = 6, fontface = "bold", family="Times New Roman")+
    labs(x = NULL, y = NULL)
  
  plot_list[[dataset]] <- p
}

# Combine all plots with 3 per row, remove legend
combined_plot <- 
  wrap_plots(plot_list, ncol = 4, nrow = 3)+
  plot_annotation(theme = theme(plot.background = element_rect(fill = "white", colour = NA))) 

# Save the combined plot
ggsave(
  filename = "/accounts/grad/aqwang/unilasso/figures_12data/figure_default_lambda/heatmap_common_features_all_datasets.png", 
  plot = combined_plot, 
  width = 18,  # Increased width for 3 columns
  height = 15, # Increased height for 4 rows
  dpi = 300
)

print(combined_plot)

```

# ignore below 
```{R}

# Define base method order
method_levels <- c("LS","Lasso","UniReg","UniLasso","Polish")


df_subset <- df_long %>%
  filter(Dataset == "CA housing") %>%
  mutate(
    Row_base = str_extract(Row, paste(method_levels, collapse = "|")),
    Col_base = str_extract(Col, paste(method_levels, collapse = "|"))
  )
row_lvls <- unique(df_subset$Row[order(match(df_subset$Row_base, method_levels))])
col_lvls <- unique(df_subset$Col[order(match(df_subset$Col_base, method_levels))])

df_subset <- df_subset %>%
  mutate(
    Row = factor(Row, levels = row_lvls),
    Col = factor(Col, levels = col_lvls)
  )

as.numeric(df_subset$Row)
```